{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Upload Gene expression database, pathway database, TF list\n",
    "all_pathways_lst= pd.read_table(\"./aracyc_pathways_input1.txt\")\n",
    "all_gene_exp = pd.read_csv(\"../../data/arabidopsis_drout_leafs/Dataset_drought_stress_seq17_2016st_136samples.csv\")\n",
    "all_tf_lst = pd.read_csv(\"../../data/arabidopsis/AT_tf_list.csv\")\n",
    "\n",
    "\n",
    "#Generate pathway and tf expression files\n",
    "PW_exp_data = pd.merge(all_pathways_lst,all_gene_exp,left_on='PW_gene',right_on='Gene')\n",
    "TF_exp_data = pd.merge(all_tf_lst,all_gene_exp,left_on=\"Gene\",right_on='Gene')\n",
    "\n",
    "PW_exp_data = PW_exp_data.drop(['PW_gene','PW_id','PW_name','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7','Unnamed: 8'], 1)\n",
    "\n",
    "PW_exp_data.drop_duplicates(subset='Gene',inplace=True)\n",
    "\n",
    "TF_exp_data.drop_duplicates(subset='Gene',inplace=True)\n",
    "\n",
    "PW_exp_data.to_csv(\"PW_exp_data.csv\")\n",
    "TF_exp_data.to_csv(\"TF_exp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Better to run this in a server\n",
    "def processInput(chunk):\n",
    "    \"\"\" This block computes MI between all pairs of PW genes and TFs. Run this part in a server and obtain the results.csv\n",
    "    file for further processing. pathway file was split to 100 gene chunks to avoid momory overflow errors. The code to \n",
    "    do chunking is shown below.\"\"\"\n",
    "    \n",
    "    tempChunkPWs = PW_exp_data.iloc[chunk]\n",
    "    combined_temp = pd.concat([tempChunkPWs, TF_exp_data])\n",
    "    combined_temp.to_csv(\"chunk\"+str(chunk[-1]+1)+\".csv\",header=False)\n",
    "    command = \"java -jar MINE.jar chunk\"+str(chunk[-1]+1)+\".csv -pairsBetween \"+str(chunk[-1]+1)+\" notify=500 >/dev/null\"\n",
    "    os.system(command)\n",
    "    os.system(\"rm chunk\"+str(chunk[-1]+1)+\".csv\")\n",
    "    os.system(\"rm chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Status.txt\")\n",
    "    temp_results_df = pd.read_csv(\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=1000],cv=0.0,B=n^0.6,Results.csv\",header=False)\n",
    "    #os.system(\"rm ./chunk\"+str(chunk[-1]+1)+\".csv,between[break=1000],cv=0.0,B=n^0.6,Results.csv\")\n",
    "    return temp_results_df\n",
    "\n",
    "\n",
    "#pw_exp_data = pd.read_csv(\"pw_exp_selected.csv\")\n",
    "#TF_exp_data = pd.read_table(\"./At_Stem_TF1622TF_uniq_exp_HW.txt\")\n",
    "data = range(PW_exp_data.shape[0])\n",
    "chunks = [data[x:x+100] for x in xrange(0, len(data), 100)]\n",
    "num_cores = 8 #multiprocessing.cpu_count()\n",
    "results = Parallel(n_jobs=num_cores)(delayed(processInput)(chunk) for chunk in chunks)\n",
    "\n",
    "results.to_csv(\"results.csv\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get a filterd dictionary of pathways with genes > 20. I analysed pathways which have atleast 20 genes.\n",
    "PW_exp = pd.read_csv(\"./pathway_exp.csv\")\n",
    "temp = PW_exp[['PW_name','PW_gene']]\n",
    "temp_dic = {pw: pwg[\"PW_gene\"].tolist() for pw,pwg in temp.groupby(\"PW_name\")}\n",
    "filtered_dict = {k:v for k,v in temp_dic.iteritems() if len(list(set(v))) > 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell to get the association dataframe in recudes PW set and all TFs\n",
    "selected_PWg = []\n",
    "for key in filtered_dict:\n",
    "    selected_PWg.append(filtered_dict[key])\n",
    "chain = itertools.chain(*selected_PWg)\n",
    "selected_PWg = set(list(chain))\n",
    "\n",
    "raw_data = {'pw_gene': list(selected_PWg)}\n",
    "\n",
    "pw_gene_df = pd.DataFrame(raw_data,columns=['pw_gene'])\n",
    "\n",
    "\n",
    "\n",
    "pw_exp_data = pd.merge(pw_gene_df,gene_exp,left_on='pw_gene',right_on='Gene')\n",
    "\n",
    "#pw_exp_data.to_csv(\"pw_exp_selected.csv\")\n",
    "pw_exp_data = pd.read_csv(\"pw_exp_selected.csv\")\n",
    "\n",
    "#pw_exp_data.drop('pw_gene', axis=1, inplace=True)\n",
    "\n",
    "data = range(pw_exp_data.shape[0])\n",
    "\n",
    "def processInput(chunk):\n",
    "    tempChunkPWs = pw_exp_data.iloc[chunk]\n",
    "    combined_temp = pd.concat([tempChunkPWs, TF_exp])\n",
    "    combined_temp.to_csv(\"chunk\"+str(chunk[-1]+1)+\".csv\",header=False,index=False)\n",
    "    #command = \"java -jar MINE.jar chunk\"+str(chunk[-1]+1)+\".csv -pairsBetween \"+str(len(chunk))+\" notify=500 >/dev/null\"\n",
    "    #os.system(command)\n",
    "    #os.system(\"rm chunk\"+str(chunk[-1]+1)+\".csv\")\n",
    "    #os.system(\"rm chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Status.txt\")\n",
    "    #temp_results_df = pd.read_csv(\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=1000],cv=0.0,B=n^0.6,Results.csv\",header=False)\n",
    "    #os.system(\"rm ./chunk\"+str(chunk[-1]+1)+\".csv,between[break=1000],cv=0.0,B=n^0.6,Results.csv\")\n",
    "\n",
    "chunks = [data[x:x+100] for x in xrange(0, len(data), 100)]\n",
    "#num_cores = 8 #multiprocessing.cpu_count()\n",
    "#results = Parallel(n_jobs=num_cores)(delayed(processInput)(chunk) for chunk in chunks)\n",
    "\n",
    "\n",
    "for chunk in chunks[0:17]:\n",
    "    print(chunk[-1]+1)\n",
    "    print (\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Results.csv\")\n",
    "    temp_results_df = pd.read_csv(\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Results.csv\")\n",
    "    pd.concat([association,temp_results_df])\n",
    "\n",
    "association.to_csv(\"all_PW_TF_association.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "association = association[association['MIC (strength)'] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_top_TFs_forPW = pd.DataFrame(columns=[u'pw_index', u'pw', u'pwg', u'X var', u'Y var', u'MIC (strength)',\n",
    "       u'MIC-p^2 (nonlinearity)', u'MAS (non-monotonicity)',\n",
    "       u'MEV (functionality)', u'MCN (complexity)', u'Linear regression (p)'])\n",
    "ind = 0\n",
    "for pw in filtered_dict:\n",
    "    #if(ind==1):\n",
    "     #   break\n",
    "    ind = ind + 1\n",
    "    print(ind,end=\",\")\n",
    "    print(pw,end=\",\")\n",
    "    print(len(filtered_dict[pw]))\n",
    "    for pwg in filtered_dict[pw]:\n",
    "        temp1 = association[association['X var']==pwg]\n",
    "        temp1 = temp1.sort_values(by='MIC (strength)',ascending=False)\n",
    "        temp1.reset_index(drop=True,inplace=True)\n",
    "        nRows = temp1.shape[0]\n",
    "        nRows\n",
    "        temp0 = pd.DataFrame([[ind]*nRows,[pw]*nRows,[pwg]*nRows]).transpose()\n",
    "        temp0.reset_index(drop=True, inplace=True)\n",
    "        temp0.columns=['pw_index','pw','pwg']\n",
    "        selected_df = pd.concat( [temp0, temp1], axis=1)\n",
    "        #selected_df2 = pd.concat([row, selected_df], axis=1,ignore_index=\"True\") THis line does not work for concat dfs !!!!\n",
    "        selected_top_TFs_forPW = pd.concat([selected_top_TFs_forPW,selected_df])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_top_TFs_forPW.to_csv(\"/Users/chathura/Desktop/MIC_cluster_Analysis/all_PW_1622TF_MIC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID2sym = pd.read_table(\"./AT_ID_SYM.txt\",header=None)\n",
    "ID2sym.columns = [\"pwg\",\"pwg_sym\"]\n",
    "temp = pd.merge(selected_top_TFs_forPW,ID2sym,on='pwg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunks = [data[x:x+100] for x in xrange(0, len(data), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "association = pd.read_csv(\"./chunk1790.csv,between[break=90],cv=0.0,B=n^0.6,Results.csv\")\n",
    "for chunk in chunks[0:17]:\n",
    "    print(chunk[-1]+1)\n",
    "    print (\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Results.csv\")\n",
    "    temp_results_df = pd.read_csv(\"./chunk\"+str(chunk[-1]+1)+\".csv,between[break=100],cv=0.0,B=n^0.6,Results.csv\")\n",
    "    association = pd.concat([association,temp_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Recreate the original PW to PWG-TF associations\n",
    "association = pd.read_csv(\"./allPW_TF_MIC_association.csv\")\n",
    "selected_top_TFs_forPW = pd.DataFrame(columns=[u'pw_index', u'pw', u'pwg', u'X var', u'Y var', u'MIC (strength)',\n",
    "       u'MIC-p^2 (nonlinearity)', u'MAS (non-monotonicity)',\n",
    "       u'MEV (functionality)', u'MCN (complexity)', u'Linear regression (p)'])\n",
    "pw_index = 0\n",
    "top_TFs = 100\n",
    "for pw in filtered_dict:\n",
    "    pw_index = pw_index + 1\n",
    "    for pwg in set(filtered_dict[pw]):\n",
    "        \n",
    "        row = pd.DataFrame([[pw_index]*top_TFs,[pw]*top_TFs,[pwg]*top_TFs]).transpose()\n",
    "        cols = ['pw_index', 'pw','pwg']\n",
    "        row.columns = cols\n",
    "        \n",
    "        temp = association[association['X var'] == pwg]\n",
    "        temp = temp.sort_values(by='MIC (strength)',ascending=False)\n",
    "        selected_df = temp.iloc[0:no_top_TFs]\n",
    "        \n",
    "        row.reset_index(drop=True, inplace=True)\n",
    "        selected_df.reset_index(drop=True, inplace=True)\n",
    "        selected_df2 = pd.concat( [row, selected_df], axis=1)\n",
    "        selected_df2 = selected_df2.dropna()\n",
    "        #selected_df2 = pd.concat([row, selected_df], axis=1,ignore_index=\"True\") THis line does not work for concat dfs !!!!\n",
    "        selected_top_TFs_forPW = pd.concat([selected_top_TFs_forPW,selected_df2])\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_top_TFs_forPW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID2sym = pd.read_table(\"./AT_ID_SYM.txt\",header=None)\n",
    "ID2sym.columns = [\"pwg\",\"pwg_sym\"]\n",
    "\n",
    "temp = pd.merge(selected_top_TFs_forPW,ID2sym,on='pwg')\n",
    "ID2sym.columns = ['Y var','tf_sym']\n",
    "final = pd.merge(temp,ID2sym,on='Y var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final.to_csv(\"/Users/chathura/Desktop/MIC_cluster_Analysis/all_PW_1622TF_MIC_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is to get the data ready for heatmap and clustering\n",
    "\n",
    "pw_df = final[final['pw_index']==1]\n",
    "tf_list = []\n",
    "for pwg in set(pw_df['pwg_sym']):\n",
    "    pwg_df = pw_df[pw_df['pwg_sym']==pwg]\n",
    "    pwg_df = pwg_df.sort_values(by='MIC (strength)',ascending=False).iloc[0:10] # find a better way to do the cutoff\n",
    "    tf_list.append(list(pwg_df['tf_sym']))\n",
    "    \n",
    "tf_list = list(set(list(itertools.chain.from_iterable(tf_list))))\n",
    "pw_list = list(set(pw_df['pwg_sym']))\n",
    "df = pd.DataFrame(index=range(0,len(tf_list)), columns=pw_list)\n",
    "#df = df.fillna(0) # with 0s rather than NaNs \n",
    "for pw in pw_list:\n",
    "    pwg_df = pw_df[pw_df['pwg_sym']==pw]\n",
    "    pwg_df = pwg_df.sort_values(by='MIC (strength)',ascending=False).iloc[0:10]\n",
    "    for tf_i in range(len(tf_list)):\n",
    "        tf_sym = tf_list[tf_i]\n",
    "        if(tf_sym in list(pwg_df['tf_sym'])):\n",
    "            idx = list(pwg_df['tf_sym']).index(tf_sym)\n",
    "            df.at[tf_i,pw] = pwg_df.iloc[idx]['MIC (strength)']\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "df = df.fillna(0)\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [prosper]",
   "language": "python",
   "name": "Python [prosper]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
